---
title: 'OptiModel'
description: 'Use Lytix to manage and call your models'
---


Using lytix to manage your [OptiModel](https://github.com/Lytix-Labs/optimodel/tree/master?tab=readme-ov-file#cloud-quickstart) server is as simple as creating a lytix API key. 

<Info>
  **Prerequisite** First create a lytix account [here](https://lab.lytix.co/home/login)
</Info>

## Create a lytix API Key

Start by creating and noting down a lytix api key. See instructions [here](/api-key-setup)

## Add a new Provider

Before you can start making LLMs calls, you'll first need to setup a new provider [here](https://lab.lytix.co/home/settings/model-providers)

![add new provider to lytix](/images/optimodel/setting-manage-providers.png)


<Info>
  **Note** Access to models is limited to what providers you have setup. For example, if you only setup OpenAI, you will not be able to call llama3 models.
</Info>

## Install the SDK

<CodeGroup>
```sh Python
pip3 install optimodel-py
```

```sh JavaScript
npm i @lytix/client
```
</CodeGroup>


## Call The SDK

Now you are ready to make your first call by passing in the `LX_API_KEY` environment variable. 

<CodeGroup>
```py Python Example
from optimodel import queryModel, listModels
from optimodel_server_types import ModelMessage, ModelTypes

prompt = "Hello How are you?"

response = await queryModel(
    model=ModelTypes.llama_3_8b_instruct,
    messages=[
        ModelMessage(
            role="system",
            content="You are a helpful assistant. Always respond in JSON syntax",
        ),
        ModelMessage(role="user", content=prompt),
    ],
)
print("Got response:", response)
```

```ts Typescript
import { ModelTypes, SpeedPriority } from "@lytix/client";
import { queryModel } from "@lytix/client";

/**
 * Simple validator to check if the response is JSON
 */
function validator(x: string): boolean {
  try {
    JSON.parse(x);
    return true;
  } catch {
    return false;
  }
}

const main = async () => {
  const prompt = "Hello How are you?";
  const response = await queryModel({
    model: ModelTypes.gpt_4o_mini,
    messages: [
      { role: "user", content: prompt },
      { role: "assistant", content: "Always respond with a JSON object" },
    ],
    speedPriority: SpeedPriority.low,
    temperature: 0.5,
    fallbackModels: [ModelTypes.gpt_4o],
    validator: validator,
    maxGenLen: 256,
  });
  console.log(`Got response: ${response}`);
};

main();
```


</CodeGroup>

Just remember to pass your `LX_API_KEY` when starting your program as an environment variable.

<CodeGroup>
```sh Python
> LX_API_KEY=<your-api-key-here> python3 your_script.py
```

```sh Typescript
> LX_API_KEY=<your-api-key-here> node your_script.js
```


</CodeGroup>



## Extra Parameters

The following extra parameters are available to pass to the `queryModel` function:

`speedPriority`: This can be used to control how OptiModel should prioritize the request. If set to `high` it will not focus on cost
<CodeGroup>
```py Python
speedPriority = "low" | "high"
```

```ts Typescript
speedPriority = "low" | "high";
```
</CodeGroup>


`validator`: This is a function that will be used to validate the response. If the validator returns `False` the request will be retried. _Note:_ You must pass `fallbackModels` if you use a validator.
<CodeGroup>
```py Python
validator = Callable[[str], bool]
```

```ts Typescript
validator = (response: string) => boolean;
```
</CodeGroup>

`fallbackModels`: This a list of other models to fallback to if the first model fails the validator.

<CodeGroup>
```py Python
fallbackModels = List[ModelTypes]
```

```ts Typescript
fallbackModels = ModelTypes[];
```
</CodeGroup>

`maxGenLen`: This is the maximum length of the response, if the model response is longer than this, it will be truncated. This will check against the contig, so for example if you pass a value of 1 million, and no provider will be able to generate a response of that length, the request will fail. 

<CodeGroup>

```py Python
maxGenLen = int
```

```ts Typescript
maxGenLen = number;
```
</CodeGroup>

`jsonMode`: This will enable json mode for the request. This is useful if you want to pass in a json object as a prompt.

<CodeGroup>
```py Python
jsonMode = bool
```

```ts Typescript
jsonMode = boolean;
```
</CodeGroup>

`provider`: You can optionally force a specific provder to be used. This is useful if you have multiple providers setup and want to force a specific one to be used.

<CodeGroup>
```py Python
provider = "openai" | "groq" | "bedrock" | "anthropic" | "together"
```

```ts Typescript
provider = "openai" | "groq" | "bedrock" | "anthropic" | "together";
```
</CodeGroup>